{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUKbcLz6cDy8qM+Vg7xoUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashank-m/covid_mining_papers/blob/master/Doc2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkjIIK9BSPHw",
        "colab_type": "text"
      },
      "source": [
        "### Imports and setting up files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfkX688ax5ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "import json\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vQSWFbzFq8s",
        "colab_type": "code",
        "outputId": "41ce19ff-9a6f-4a94-9413-978aa88a6944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIgG-tkXu0W2",
        "colab_type": "code",
        "outputId": "a0f84732-ed7f-4dbe-e4e5-49dfb23fcadb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe62fd77bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFUYtfQ2vWvm",
        "colab_type": "code",
        "outputId": "87dc3b95-8510-4053-a039-6023290ae0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPmdPOarxzpl",
        "colab_type": "code",
        "outputId": "cfabaa48-7f82-4baf-8b74-3bbf82343dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GStG0DA1Mr-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filenames=os.listdir(\"/content/drive/My Drive/covid_nlp/2020-03-13/biorxiv_medrxiv/biorxiv_medrxiv\")\n",
        "biorxiv=\"/content/drive/My Drive/covid_nlp/2020-03-13/biorxiv_medrxiv/biorxiv_medrxiv/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LM5Ha1sRScR",
        "colab_type": "text"
      },
      "source": [
        "## ***Preprocessing of abstract***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPXYtEecsWI_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQ02m2Z-Q8vp",
        "colab": {}
      },
      "source": [
        "all_files=[json.load(open(biorxiv+filename, 'rb')) for filename in filenames]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZAI-MrH-ezyv",
        "colab": {}
      },
      "source": [
        "def preprocess(abstract):\n",
        "  abstract=re.sub(r'[,-?<>:!_+=#$()%^{}|~@;\\'\"*&[\\]]',' ',abstract) #remove punctuation.\n",
        "  abstract=re.sub(r'\\d+',' ',abstract) # remove digits.\n",
        "  abstract=re.sub(r'\\s+',' ',abstract).strip() # replace multiple white spaces with single whiite space.\n",
        "  return abstract"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K748TrxnLL6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_abstracts=[]\n",
        "for i,f in enumerate(all_files):\n",
        "  if f['abstract']: # checks if abstract exists or not.\n",
        "    abstract=''\n",
        "\n",
        "    for dic in f['abstract']:\n",
        "      abstract+=dic['text']\n",
        "      abstract+='\\n'\n",
        "    abstract=abstract.lower() # converts all alphabets to lower case.\n",
        "    abstract=preprocess(abstract)\n",
        "\n",
        "    if abstract!='' and len(re.findall(r'\\S+',abstract))>=5: # rejects abstracts which are empty or less than 5 words. \n",
        "      cleaned_abstracts.append((abstract,i)) # appending a tuple of abstract and paper number. \n",
        "no_docs=len(cleaned_abstracts)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXA_8NeXgAAo",
        "colab_type": "text"
      },
      "source": [
        "### **Create train and test data for Doc2Vec model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfGyU0WN9yrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v=''\n",
        "for doc,i in cleaned_abstracts:\n",
        "  v+=doc\n",
        "v=v.split()\n",
        "vocab=set(v)\n",
        "vocab_size=len(vocab)\n",
        "word_to_idx={}\n",
        "for i,word in enumerate(vocab):\n",
        "  word_to_idx[word]=i\n",
        "word_to_idx['<UNK>']=vocab_size  \n",
        "vocab_size+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBqaYH-z0AqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "total_input=[]\n",
        "total_lable=[]\n",
        "i=0\n",
        "for doc,file_num in (cleaned_abstracts):\n",
        "  if doc:\n",
        "    train_data=[]\n",
        "    all_tokens=re.findall(r'\\S+',doc)\n",
        "    penta_gram=ngrams(all_tokens,5)\n",
        "\n",
        "    for window in penta_gram:\n",
        "      window=list(window)\n",
        "      test_word=window[2]\n",
        "      train_words=window[:2]+window[3:]\n",
        "      train_words.append(i)\n",
        "      train_data.append((train_words,test_word)) \n",
        "\n",
        "    context_data=[]\n",
        "    target_data=[]\n",
        "\n",
        "    for context,target in train_data:\n",
        "\n",
        "      inputs=[]\n",
        "      for word in context[:-1]:\n",
        "        try:\n",
        "          inputs.append(word_to_idx[word])\n",
        "        except KeyError:\n",
        "          inputs.append(word_to_idx['<UNK>'])\n",
        "      inputs.append(i)   # i is the document number.   \n",
        "      inputs.append(file_num) # file_num is the file in biorxiv folder. They may not be same as some files were removed due to lack of abstract.\n",
        "      inputs=torch.LongTensor(inputs).view(1,-1)\n",
        "      target_vector=torch.LongTensor([word_to_idx[target]])\n",
        "      context_data.append(inputs)\n",
        "      target_data.append(target_vector)\n",
        "\n",
        "    try:\n",
        "      X_train=torch.cat(context_data,0)\n",
        "      X_train_label=torch.cat(target_data,0)\n",
        "      total_input.append(X_train)\n",
        "      total_lable.append(X_train_label)\n",
        "      i+=1\n",
        "    except RuntimeError:\n",
        "      print(i,file_num)\n",
        "      pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCt0DotFLw5C",
        "colab_type": "text"
      },
      "source": [
        "**Last column of X_train contains file_num and last but one contains document no.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doiANoKt9Kqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=torch.cat(total_input,0) # put all training_data into a single tensor so that it can be batched by Dataloader."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d55HfFViAMmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_label=torch.cat(total_lable,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liGM4_aPBJIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=64\n",
        "training=TensorDataset(X_train,X_train_label)\n",
        "train_loader=DataLoader(training,batch_size=batch_size,drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1vFYaM5Ph-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73e5813c-d76e-4e79-eb2f-d31589752fa7"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([212592, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pLSxnUHRtSF",
        "colab_type": "text"
      },
      "source": [
        "### Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q6EcYdVNy1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cbow(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,context,hidden_dim,batch_size,no_docs):\n",
        "    super(cbow,self).__init__()\n",
        "    self.ndocs=no_docs\n",
        "    self.bs=batch_size\n",
        "    self.c=context\n",
        "\n",
        "    self.w_embeddings=nn.Embedding(vocab_size,embed_dim)\n",
        "    self.d_embeddings=nn.Embedding(no_docs,embed_dim)\n",
        "\n",
        "    self.linear1=nn.Linear(embed_dim,hidden_dim)\n",
        "    self.linear2=nn.Linear(hidden_dim,vocab_size)\n",
        "    \n",
        "  def forward(self,inputs):\n",
        "      \n",
        "      w_embeds=self.w_embeddings(inputs[:,:4])\n",
        "      d_embeds=self.d_embeddings(inputs[:,4]).unsqueeze(1)\n",
        "      combined=torch.cat((w_embeds,d_embeds),1)\n",
        "      a_1=F.relu(self.linear1(combined).sum(axis=1)/(self.c+1))\n",
        "      out=self.linear2(a_1)\n",
        "      return out\n",
        "\n",
        "embed_dim=300\n",
        "model=cbow(vocab_size,embed_dim,4,50,batch_size,no_docs)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "lr=1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.state_dict\n",
        "if(is_cuda):\n",
        "  model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q7-Amdneu2v",
        "colab_type": "text"
      },
      "source": [
        "**Note that here training data is generated from all abstracts\\.We run the Doc2Vec model over 3 epochs and it looks like the model is learning something as the loss decreases every epoch. If we can overfit on this small data that means out training is not an issue.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-sN2ccJYreu",
        "colab_type": "code",
        "outputId": "c9cc9457-f3a1-4893-c965-528837a21a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "epochs=3\n",
        "for j in range(epochs):\n",
        "  train_loss=[]\n",
        "  for i,(x,y) in enumerate(train_loader):\n",
        "    count+=1\n",
        "    model.zero_grad()\n",
        "    if(is_cuda):\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "    out=model(x)\n",
        "\n",
        "    loss=criterion(out,y)\n",
        "    loss.backward()\n",
        "\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "    optimizer.step()\n",
        "    if i%200==0:\n",
        "      print('epoch {},batch={},running train loss={}'.format(j+1,i,np.average(train_loss)))    "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1,batch=0,running train loss=9.55390453338623\n",
            "epoch 1,batch=200,running train loss=8.496730541115376\n",
            "epoch 1,batch=400,running train loss=7.944065985834212\n",
            "epoch 1,batch=600,running train loss=7.73745178343254\n",
            "epoch 1,batch=800,running train loss=7.603608276662458\n",
            "epoch 1,batch=1000,running train loss=7.521674966955042\n",
            "epoch 1,batch=1200,running train loss=7.457838540470273\n",
            "epoch 1,batch=1400,running train loss=7.395984579204748\n",
            "epoch 1,batch=1600,running train loss=7.338700695085496\n",
            "epoch 1,batch=1800,running train loss=7.2907657408833435\n",
            "epoch 1,batch=2000,running train loss=7.262255778972772\n",
            "epoch 1,batch=2200,running train loss=7.240111396833747\n",
            "epoch 1,batch=2400,running train loss=7.222505417628767\n",
            "epoch 1,batch=2600,running train loss=7.193835469586535\n",
            "epoch 1,batch=2800,running train loss=7.170622033844757\n",
            "epoch 1,batch=3000,running train loss=7.153190809978878\n",
            "epoch 1,batch=3200,running train loss=7.137829578656474\n",
            "epoch 2,batch=0,running train loss=6.237630844116211\n",
            "epoch 2,batch=200,running train loss=6.2320886109005755\n",
            "epoch 2,batch=400,running train loss=6.226714096759026\n",
            "epoch 2,batch=600,running train loss=6.268258995303695\n",
            "epoch 2,batch=800,running train loss=6.2578687602363425\n",
            "epoch 2,batch=1000,running train loss=6.279686967809718\n",
            "epoch 2,batch=1200,running train loss=6.289892215117328\n",
            "epoch 2,batch=1400,running train loss=6.290026311105188\n",
            "epoch 2,batch=1600,running train loss=6.2790523325927845\n",
            "epoch 2,batch=1800,running train loss=6.27491544472516\n",
            "epoch 2,batch=2000,running train loss=6.2806578639267325\n",
            "epoch 2,batch=2200,running train loss=6.286944531896124\n",
            "epoch 2,batch=2400,running train loss=6.2968918338809\n",
            "epoch 2,batch=2600,running train loss=6.292935334549918\n",
            "epoch 2,batch=2800,running train loss=6.293271111148206\n",
            "epoch 2,batch=3000,running train loss=6.297887699876536\n",
            "epoch 2,batch=3200,running train loss=6.303327425685907\n",
            "epoch 3,batch=0,running train loss=5.820108413696289\n",
            "epoch 3,batch=200,running train loss=5.823762008799842\n",
            "epoch 3,batch=400,running train loss=5.832661480083132\n",
            "epoch 3,batch=600,running train loss=5.868072779523751\n",
            "epoch 3,batch=800,running train loss=5.855921765838223\n",
            "epoch 3,batch=1000,running train loss=5.874074860052629\n",
            "epoch 3,batch=1200,running train loss=5.882111373789404\n",
            "epoch 3,batch=1400,running train loss=5.8794018025061305\n",
            "epoch 3,batch=1600,running train loss=5.8641788159066035\n",
            "epoch 3,batch=1800,running train loss=5.8599880724599265\n",
            "epoch 3,batch=2000,running train loss=5.861621474397594\n",
            "epoch 3,batch=2200,running train loss=5.865864033213749\n",
            "epoch 3,batch=2400,running train loss=5.8751203028274945\n",
            "epoch 3,batch=2600,running train loss=5.871305630922959\n",
            "epoch 3,batch=2800,running train loss=5.872575088170374\n",
            "epoch 3,batch=3000,running train loss=5.878677418890574\n",
            "epoch 3,batch=3200,running train loss=5.8862110479888745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdDMH16Sl8HE",
        "colab_type": "text"
      },
      "source": [
        "**As we can see above, our tarin loss almost comes down from 9.5 to around 5.8 in 3 epochs. Our model is learning. Val set can be added to check generalising ability and hyperparam tuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Ir7SHva7AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}