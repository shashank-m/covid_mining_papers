{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_doc2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOb+sizfuYFKHylYHx3rKem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashank-m/covid_mining_papers/blob/master/word_doc2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfkX688ax5ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.util import ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vQSWFbzFq8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9169f966-44be-4e9c-b0b1-5d54a3f96800"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIgG-tkXu0W2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67c3145f-e239-4763-be5b-40d75312fc1f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9740666c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFUYtfQ2vWvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "110cd6ee-ded2-4113-9487-cd72ea6d946b"
      },
      "source": [
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU not available, CPU used\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPmdPOarxzpl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ba9618c4-c071-4426-efa9-e5b9a3443e72"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjSbmkGVEybr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentence = \"\"\"<UNK> <UNK> When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold. <UNK> <UNK>\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr95QibjFh8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_data=[]\n",
        "# train_labels=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbgcag1VFnWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CBOW model.\n",
        "import re\n",
        "all_tokens=re.findall(r'\\S+',test_sentence)\n",
        "penta_gram=ngrams(all_tokens,5)\n",
        "for window in penta_gram:\n",
        "  window=list(window)\n",
        "  test_word=window[2]\n",
        "  train_words=window[:2]+window[3:]\n",
        "  train_data.append((train_words,test_word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3RRe-lsMUco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "vocab=set(all_tokens)\n",
        "vocab_size=len(set(all_tokens))\n",
        "word_to_idx={}\n",
        "for i,word in enumerate(vocab):\n",
        "  word_to_idx[word]=i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3BhJo867ytk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_data=[]\n",
        "target_data=[]\n",
        "for context,target in train_data:\n",
        "  inputs=torch.LongTensor([word_to_idx[word] for word in context]).view(1,-1) \n",
        "  target_vector=torch.LongTensor([word_to_idx[target]])\n",
        "  context_data.append(inputs)\n",
        "  target_data.append(target_vector)\n",
        "# train_data  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doiANoKt9Kqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=torch.cat(context_data,0) # put all training_data into a single tensor so that it can be batched by Dataloader."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d55HfFViAMmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_label=torch.cat(target_data,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liGM4_aPBJIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=1\n",
        "training=TensorDataset(X_train,X_train_label)\n",
        "train_loader=DataLoader(training,batch_size=batch_size,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q6EcYdVNy1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4e6e608b-c33c-42b7-8873-c78ca0a41f3d"
      },
      "source": [
        "class cbow(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,context,hidden_dim,batch_size):\n",
        "    super(cbow,self).__init__()\n",
        "    self.bs=batch_size\n",
        "    self.c=context\n",
        "    self.embeddings=nn.Embedding(vocab_size,embed_dim)\n",
        "    self.linear1=nn.Linear(embed_dim,hidden_dim)\n",
        "    self.linear2=nn.Linear(hidden_dim,vocab_size)\n",
        "    \n",
        "  def forward(self,inputs):\n",
        "    # if inputs.shape[0]==self.bs:\n",
        "      embeds=self.embeddings(inputs)\n",
        "      a_1=F.relu(self.linear1(embeds).sum(axis=1)/self.c)\n",
        "      out=self.linear2(a_1)\n",
        "      return out\n",
        "\n",
        "embed_dim=300\n",
        "model=cbow(vocab_size,embed_dim,4,50,batch_size)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "lr=1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.state_dict"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of cbow(\n",
              "  (embeddings): Embedding(98, 300)\n",
              "  (linear1): Linear(in_features=300, out_features=50, bias=True)\n",
              "  (linear2): Linear(in_features=50, out_features=98, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgd_6FcZgZTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.embeddings(torch.LongTensor([2])) # embedding of the word winters before training."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q7-Amdneu2v",
        "colab_type": "text"
      },
      "source": [
        "**Note that here training data is generated from only one sentence.We run the cbow model over 5 epochs and it looks like the model is learning something as the loss decreases every epoch. If we can overfit on this small data that means out training process works. We just need to collect more training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-sN2ccJYreu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6f0dd33e-1c36-4994-c5b7-f2d7a9eaeb5e"
      },
      "source": [
        "epochs=20\n",
        "for j in range(epochs):\n",
        "  for i,(x,y) in enumerate(train_loader):\n",
        "    model.zero_grad()\n",
        "    out=model(x)\n",
        "\n",
        "    loss=criterion(out,y)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "  print('After epoch {},loss={}'.format(j+1,loss))    "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After epoch 1,loss=4.686923027038574\n",
            "After epoch 2,loss=3.713883876800537\n",
            "After epoch 3,loss=2.426980495452881\n",
            "After epoch 4,loss=0.8109285235404968\n",
            "After epoch 5,loss=0.17440193891525269\n",
            "After epoch 6,loss=0.10496030002832413\n",
            "After epoch 7,loss=0.06620841473340988\n",
            "After epoch 8,loss=0.047774963080883026\n",
            "After epoch 9,loss=0.036644503474235535\n",
            "After epoch 10,loss=0.029920388013124466\n",
            "After epoch 11,loss=0.023427007719874382\n",
            "After epoch 12,loss=0.01968187279999256\n",
            "After epoch 13,loss=0.016636686399579048\n",
            "After epoch 14,loss=0.01425135973840952\n",
            "After epoch 15,loss=0.012160095386207104\n",
            "After epoch 16,loss=0.010611655190587044\n",
            "After epoch 17,loss=0.009309808723628521\n",
            "After epoch 18,loss=0.0081344498321414\n",
            "After epoch 19,loss=0.007255277596414089\n",
            "After epoch 20,loss=0.006278789136558771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdDMH16Sl8HE",
        "colab_type": "text"
      },
      "source": [
        "**As we can see above, our tarin loss almost comes down to zero. This shows the learning process is not an issue. Time to collect more trainig data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUP2eqPYlFwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}